# -*- coding: utf-8 -*-
"""Function _4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MIumi_h3OTeLGlvIgdHbsZMTgo_UUchv
"""

from google.colab import drive
drive.mount('/content/drive')

"""## **Load data and Exploration**"""

# Import necessary libraries for data analysis and visualization
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# reads in a CSV file
data = pd.read_csv('/content/drive/MyDrive/Fish2.csv')

data

# selects a random sample of 5 rows
data.sample(5)

# the number of rows and the number of columns in the DataFrame
data.shape

# Generate a statistical summary of a dataset
data.describe()

"""## **Data Preprocessing**"""

# summary of the Dataset
data.info()

#Finding Missing Values
data.isnull().sum()

#Finding Duplicate Rows
duplicate_rows = data[data.duplicated()]
duplicate_rows

# List of column names to remove
unwanted_columns = ['created_at','entry_id','Fish_Length(cm)','Fish_Weight(g)']
# Remove the unwanted columns using drop
data = data.drop(columns=unwanted_columns)

data

"""The dataset is clean, with no missing values or duplicate entries."""

# Histograms to visualize the distribution of each feature to identify potential outliers
data.hist(figsize=(20, 10), bins=25)
plt.show()

"""## **Exploratory Data Analysis (EDA):**

### Categorical Feature Analysis
"""

# retrieves the count of unique values in the 'Fish Diseases' column
data['Fish Diseases'].value_counts()

# creates a pie chart to show the proportion of unique values in the 'Fish Diseases' column
plt.pie(data['Fish Diseases'].value_counts(),autopct="%1.1f%%",labels=['Red Spot','Fin Rot','White Spot (Ich)'])
plt.legend()

sns.countplot(x = data["Fish Diseases"], palette="Purples")
plt.title("Number of Fish Diseases in each Class",fontsize=10)
plt.show()
# visualize the distribution of the 'class' column

"""After examining the distribution of the target variable, we can assume that the classes are balanced"""

#grid of scatterplots (pairwise plots) that visualize the relationships between multiple variables in a dataset
sns.pairplot(data,hue='Fish Diseases')

data.plot(kind='box', figsize=(20, 10))
plt.show()

"""Detect outliers"""

mean = np.mean(data['Ammonia(g/ml)'])
mean

std = np.std(data['Ammonia(g/ml)'])
std

(data['Ammonia(g/ml)']-mean)/std

data['Ammonia(g/ml)_z_score'] = (data['Ammonia(g/ml)'] - mean)/std
data.head(5)

data[data['Ammonia(g/ml)_z_score']>3]

data[data['Ammonia(g/ml)_z_score']<-3]

data['Ammonia(g/ml)_z_score'].min

"""Remove **outliers**"""

outlier_indexes = []

outlier_indexes.extend(data.index[data['Ammonia(g/ml)_z_score']>3].tolist())

outlier_indexes

outlier_indexes.extend(data.index[data['Ammonia(g/ml)_z_score']<-3].tolist())

outlier_indexes

data = data.drop(data.index[outlier_indexes])
data.head(5)

data.shape

data = data.drop('Ammonia(g/ml)_z_score', axis=1)

data.shape

data

"""# **Feature Engineering**

Feature Selection
"""

# creates a heatmap using seaborn library to visualize the correlation between the columns of the 'data'
plt.figure(figsize = (18,10))
sns.heatmap(data.corr(), annot = True, fmt = ".1f", linewidths = 1.9)
plt.show()

"""Feature Scaling(Standardization)"""

x = data.drop('Fish Diseases',axis=1)

x

y = data['Fish Diseases']

y

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
scaled_data = scaler.fit_transform(x)

scaled_data

"""## Data Splitting


"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.30)

"""## Model Selection

SVM
"""

from sklearn.model_selection import cross_val_score

from sklearn.svm import SVC
svm = SVC()
cross_val_score(svm,x_train,y_train,cv=5).mean()

"""## Naive Bayes"""

from sklearn.naive_bayes import GaussianNB
nb = GaussianNB()
cross_val_score(nb,x_train,y_train,cv=5).mean()

"""## Random Forest"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier()
cross_val_score(rf,x_train,y_train,cv=5).mean()

"""## KNN"""

from sklearn.neighbors import KNeighborsClassifier
knn = KNeighborsClassifier()
cross_val_score(knn,x_train,y_train,cv=5).mean()

"""## Model Training"""

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=50)
rf.fit(x_train,y_train)

pred = rf.predict(x_test)
pred

y_test

from sklearn.metrics import accuracy_score
accuracy_score(pred,y_test)

"""For scaled data"""

from sklearn.model_selection import train_test_split
x_train, x_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.30)

from sklearn.ensemble import RandomForestClassifier
rf = RandomForestClassifier(n_estimators=50)
rf.fit(x_train,y_train)

pred = rf.predict(x_test)
pred

y_test

from sklearn.metrics import accuracy_score
accuracy_score(pred,y_test)

"""## **Hyperparameter Tuning**"""

from sklearn.model_selection import GridSearchCV
# Create the parameter grid based on the results of random search
param_grid = {
    'bootstrap': [True],
    'max_depth': [80, 90, 100, 110],
    'max_features': [2, 3],
    'min_samples_leaf': [3, 4, 5],
    'min_samples_split': [8, 10, 12],
    'n_estimators': [100, 200, 300, 1000]
}
# Create a based model
rf = RandomForestClassifier()
# Instantiate the grid search model
grid_search = GridSearchCV(estimator = rf, param_grid = param_grid,
                          cv = 3, n_jobs = -1, verbose = 2)

# Fit the grid search to the data
grid_search.fit(x_train,y_train)
grid_search.best_params_

# After grid search is complete, you can access the best hyperparameters
best_hyperparameters = grid_search.best_params_

from sklearn.ensemble import RandomForestClassifier  # Replace with the appropriate model class

# Create a new model with the best hyperparameters
best_model = RandomForestClassifier(**best_hyperparameters)

# Fit the best model to the training data
best_model.fit(x_train,y_train)

from sklearn.metrics import accuracy_score

# Use the best model to make predictions on the test data
y_pred = best_model.predict(x_test)

# Calculate the accuracy
accuracy = accuracy_score(y_test,y_pred)

# Print the accuracy
print(f"Accuracy of the best model: {accuracy}")

"""# Model Evaluation

confusion **matrix**
"""

from sklearn.metrics import confusion_matrix, classification_report

confusion = confusion_matrix(y_test,y_pred)
print("Confusion Matrix:")
print(confusion)

# Create a heatmap of the confusion matrix
plt.figure(figsize=(8, 6))
sns.heatmap(confusion, annot=True, fmt='d', cmap='Blues', cbar=False)

# Set axis labels
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')

# Display the plot
plt.show()

# Generate a classification report
classification_rep = classification_report(y_test, y_pred)
print("Classification Report:")
print(classification_rep)